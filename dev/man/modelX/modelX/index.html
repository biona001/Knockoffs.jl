<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Model-X Knockoffs · Knockoffs.jl</title><meta name="title" content="Model-X Knockoffs · Knockoffs.jl"/><meta property="og:title" content="Model-X Knockoffs · Knockoffs.jl"/><meta property="twitter:title" content="Model-X Knockoffs · Knockoffs.jl"/><meta name="description" content="Documentation for Knockoffs.jl."/><meta property="og:description" content="Documentation for Knockoffs.jl."/><meta property="twitter:description" content="Documentation for Knockoffs.jl."/><script data-outdated-warner src="../../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../../assets/documenter.js"></script><script src="../../../search_index.js"></script><script src="../../../siteinfo.js"></script><script src="../../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../../">Knockoffs.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../../">Home</a></li><li><a class="tocitem" href="../../fixed/fixed/">Fixed-X Knockoffs</a></li><li class="is-active"><a class="tocitem" href>Model-X Knockoffs</a><ul class="internal"><li><a class="tocitem" href="#Gaussian-model-X-knockoffs-with-known-mean-and-covariance"><span>Gaussian model-X knockoffs with known mean and covariance</span></a></li><li><a class="tocitem" href="#Second-order-knockoffs"><span>Second order knockoffs</span></a></li><li><a class="tocitem" href="#Approximate-construction-for-speed"><span>Approximate construction for speed</span></a></li><li><a class="tocitem" href="#Multiple-knockoffs"><span>Multiple knockoffs</span></a></li><li><a class="tocitem" href="#LASSO-example"><span>LASSO example</span></a></li></ul></li><li><a class="tocitem" href="../../group/">Group Knockoffs</a></li><li><a class="tocitem" href="../../knockoffscreen/knockoffscreen/">KnockoffScreen Knockoffs</a></li><li><a class="tocitem" href="../../ghost_knockoffs/">Ghost Knockoffs</a></li><li><a class="tocitem" href="../../hmm/hmm/">HMM Knockoffs</a></li><li><a class="tocitem" href="../../ipad/">IPAD Knockoffs</a></li><li><a class="tocitem" href="../../JuliaCall/">Calling from R/Python</a></li><li><a class="tocitem" href="../../api/">API</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Model-X Knockoffs</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Model-X Knockoffs</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/biona001/Knockoffs.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/biona001/Knockoffs.jl/blob/master/docs/src/man/modelX/modelX.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Model-X-knockoffs"><a class="docs-heading-anchor" href="#Model-X-knockoffs">Model-X knockoffs</a><a id="Model-X-knockoffs-1"></a><a class="docs-heading-anchor-permalink" href="#Model-X-knockoffs" title="Permalink"></a></h1><p>This tutorial is for generating model-X (Gaussian) knockoffs, which handles cases where covariates outnumber sample size (<span>$p &gt; n$</span>). The methodology is described in the following paper</p><blockquote><p>Candes E, Fan Y, Janson L, Lv J. <em>Panning for gold:‘model‐X’knockoffs for high dimensional controlled variable selection.</em> Journal of the Royal Statistical Society: Series B (Statistical Methodology). 2018 Jun;80(3):551-77.</p></blockquote><pre><code class="language-julia hljs"># load packages needed for this tutorial
using Knockoffs
using Random
using GLMNet
using Distributions
using LinearAlgebra
using ToeplitzMatrices
using StatsKit
using Plots
gr(fmt=:png);</code></pre><h2 id="Gaussian-model-X-knockoffs-with-known-mean-and-covariance"><a class="docs-heading-anchor" href="#Gaussian-model-X-knockoffs-with-known-mean-and-covariance">Gaussian model-X knockoffs with known mean and covariance</a><a id="Gaussian-model-X-knockoffs-with-known-mean-and-covariance-1"></a><a class="docs-heading-anchor-permalink" href="#Gaussian-model-X-knockoffs-with-known-mean-and-covariance" title="Permalink"></a></h2><p>To illustrate, lets simulate data <span>$\mathbf{X}$</span> with covariance <span>$\Sigma$</span> and mean <span>$\mu$</span>. Our model is</p><p class="math-container">\[\begin{aligned}
    X_{p \times 1} \sim N(\mathbf{0}_p, \Sigma)
\end{aligned}\]</p><p>where</p><p class="math-container">\[\begin{aligned}
\Sigma = 
\begin{pmatrix}
    1 &amp; \rho &amp; \rho^2 &amp; ... &amp; \rho^p\\
    \rho &amp; 1 &amp; &amp; ... &amp; \rho^{p-1}\\
    \vdots &amp; &amp; &amp; 1 &amp; \vdots \\
    \rho^p &amp; \cdots &amp; &amp; &amp; 1
\end{pmatrix}
\end{aligned}\]</p><p>Given <span>$n$</span> iid samples from the above distribution, we will generate knockoffs according to </p><p class="math-container">\[\begin{aligned}
(X, \tilde{X}) \sim N
\left(0, \ 
\begin{pmatrix}
    \Sigma &amp; \Sigma - diag(s)\\
    \Sigma - diag(s) &amp; \Sigma
\end{pmatrix}
\right)
\end{aligned}\]</p><p>where <span>$s$</span> is solved so that <span>$0 \le s_j \le \Sigma_{jj}$</span> for all <span>$j$</span> and <span>$2Σ - diag(s)$</span> is PSD. </p><pre><code class="language-julia hljs">Random.seed!(2022)
n = 500 # sample size
p = 1000 # number of covariates
ρ = 0.4
Σ = Matrix(SymmetricToeplitz(ρ.^(0:(p-1)))) # true covariance matrix
μ = zeros(p) # true mean parameters
L = cholesky(Σ).L
X = randn(n, p) * L # var(X) = L var(N(0, 1)) L&#39; = var(Σ)</code></pre><pre><code class="nohighlight hljs">500×1000 Matrix{Float64}:
 -0.255643    0.12145      1.90832   …  -0.425334  -0.0875185  -1.26044
  1.21857    -1.04975     -1.93608       0.986266   0.495375    0.526645
 -0.489054   -0.325137    -0.389752     -0.54062   -0.765207   -0.925541
  1.13077     0.715132     0.115053     -0.866809   0.835603    1.41018
 -2.06667    -0.799976     0.104784     -1.10473   -1.53618    -1.48403
 -0.692878   -1.04012     -0.711309  …   0.117786   0.419314    1.05
  0.605767   -0.220341    -0.62107       1.36572   -0.454627   -0.226038
 -0.156307    0.0225261   -0.117329      1.06143    1.35028     1.0699
  0.443743    2.41354      0.635028      0.744278   0.229644   -0.640157
  0.710929    0.0527427    1.35858       1.06147   -0.142669   -1.67164
  0.785485    1.72134     -1.02638   …  -0.222289  -0.903092   -0.237564
 -0.0330742   1.02192      0.367135     -0.412167   0.127533   -0.0828143
 -2.01006    -0.858529    -0.817414      1.52695    1.67114     2.15544
  ⋮                                  ⋱                         
 -0.324703    0.476295     0.106425     -1.06599   -1.88418    -1.02433
 -0.811388    0.00190805  -1.16822       0.780591   1.11014    -0.208461
 -0.184579    0.344966    -0.648001  …   1.21303   -0.403468   -2.11791
  1.27172     2.03987      1.4584       -0.819745   0.0938613   0.114038
 -0.688407    0.0815265   -0.503051      0.283407  -1.10525     0.131074
 -0.892244   -0.184611    -0.746692     -0.87555   -2.00235    -0.291364
  1.57011     0.315036     1.35995       0.582807  -0.68021    -1.27912
 -0.503994   -1.70271     -0.186807  …  -0.67245   -1.07302    -0.755238
 -0.437047    0.27435     -0.821421     -1.33403   -0.368807   -0.0284317
 -2.81068    -0.361046     1.19981      -1.29837   -0.151723    1.00562
 -1.54038     0.403661     0.545421      0.728631  -1.2155      0.577002
  0.194411    0.885717     0.54569      -0.753762  -1.55452    -0.416219</code></pre><p>To generate model-X knockoffs,</p><ul><li>The 4 argument function <a href="https://biona001.github.io/Knockoffs.jl/dev/man/api/#Knockoffs.modelX_gaussian_knockoffs"><code>modelX_gaussian_knockoffs</code></a> will generate exact model-X knockoffs. </li><li>First argument is the design matrix <code>X</code>. </li><li>The second argument specifies the optimization method to generate knockoffs. We recommend <code>:mvr</code> or <code>:maxent</code> because they are <a href="https://projecteuclid.org/journals/annals-of-statistics/volume-50/issue-1/Powerful-knockoffs-via-minimizing-reconstructability/10.1214/21-AOS2104.short">more efficient to compute and tend to be more powerful than the SDP construction</a>. </li><li>The 3rd and 4th argument supplies the true mean and covariance of features.</li></ul><pre><code class="language-julia hljs"># for larger problems, consider including `verbose=true` argument to monitor convergence
@time equi = modelX_gaussian_knockoffs(X, :equi, μ, Σ)
@time mvr = modelX_gaussian_knockoffs(X, :mvr, μ, Σ)
@time me = modelX_gaussian_knockoffs(X, :maxent, μ, Σ);</code></pre><pre><code class="nohighlight hljs">  8.064953 seconds (17.21 M allocations: 933.741 MiB, 1.84% gc time, 99.02% compilation time)
  3.375101 seconds (59 allocations: 100.117 MiB, 1.45% gc time)
  1.909561 seconds (57 allocations: 100.102 MiB)</code></pre><p>The return type is a <code>GaussianKnockoff</code> struct, which contains the following fields</p><pre><code class="language-julia hljs">struct GaussianKnockoff{T&lt;:AbstractFloat, M&lt;:AbstractMatrix, S &lt;: Symmetric} &lt;: Knockoff
    X::M # n × p design matrix
    Xko::Matrix{T} # n × mp knockoff of X
    s::Vector{T} # p × 1 vector. Diagonal(s) and 2Sigma - Diagonal(s) are both psd
    Sigma::S # p × p symmetric covariance matrix. 
    method::Symbol # method for solving s
    m::Int # number of knockoffs per feature generated
end</code></pre><p>Thus, to access these fields, one can do e.g.</p><pre><code class="language-julia hljs">s = mvr.s</code></pre><pre><code class="nohighlight hljs">1000-element Vector{Float64}:
 0.7055844308562433
 0.550600272751968
 0.5579639876207405
 0.5578996993527637
 0.557883689440236
 0.5578849315291078
 0.5578848927508967
 0.5578848919247213
 0.5578848920850612
 0.557884892075635
 0.557884892074806
 0.5578848920738697
 0.5578848920729174
 ⋮
 0.5578848730690327
 0.5578848733890451
 0.5578848767064315
 0.5578848743329564
 0.5578848745291356
 0.557884875752427
 0.5578849142012816
 0.5578836722536301
 0.5578996821189737
 0.557963970313357
 0.5506002575404055
 0.7055843980219556</code></pre><pre><code class="language-julia hljs"># compare s values for different methods
[me.s mvr.s equi.s]</code></pre><pre><code class="nohighlight hljs">1000×3 Matrix{Float64}:
 0.760607  0.705584  0.857145
 0.599795  0.5506    0.857145
 0.611403  0.557964  0.857145
 0.610539  0.5579    0.857145
 0.610604  0.557884  0.857145
 0.610599  0.557885  0.857145
 0.610599  0.557885  0.857145
 0.610599  0.557885  0.857145
 0.610599  0.557885  0.857145
 0.610599  0.557885  0.857145
 0.610599  0.557885  0.857145
 0.610599  0.557885  0.857145
 0.610599  0.557885  0.857145
 ⋮                   
 0.610599  0.557885  0.857145
 0.610599  0.557885  0.857145
 0.610599  0.557885  0.857145
 0.610599  0.557885  0.857145
 0.610599  0.557885  0.857145
 0.610599  0.557885  0.857145
 0.610599  0.557885  0.857145
 0.610603  0.557884  0.857145
 0.610539  0.5579    0.857145
 0.611403  0.557964  0.857145
 0.599795  0.5506    0.857145
 0.760607  0.705584  0.857145</code></pre><h2 id="Second-order-knockoffs"><a class="docs-heading-anchor" href="#Second-order-knockoffs">Second order knockoffs</a><a id="Second-order-knockoffs-1"></a><a class="docs-heading-anchor-permalink" href="#Second-order-knockoffs" title="Permalink"></a></h2><p>In practice, one usually do not have access to true mean <code>\mu</code> and covariance <code>\Sigma</code>. Thus, we provide routines to estimate them from data. In our software, the covariance is approximated by a shrinkage method (default = ledoit wolf) rather than using the sample covariance, see API for detail. </p><p>The 2 argument <a href="https://biona001.github.io/Knockoffs.jl/dev/man/api/#Knockoffs.modelX_gaussian_knockoffs"><code>modelX_gaussian_knockoffs</code></a> will estimate the mean and covariance of <code>X</code> and use them to generate model-X knockoffs</p><pre><code class="language-julia hljs"># make 2nd order knockoffs
@time me_2nd_order = modelX_gaussian_knockoffs(X, :maxent);</code></pre><pre><code class="nohighlight hljs">  2.210275 seconds (3.37 M allocations: 317.416 MiB, 3.50% gc time, 24.69% compilation time)</code></pre><h2 id="Approximate-construction-for-speed"><a class="docs-heading-anchor" href="#Approximate-construction-for-speed">Approximate construction for speed</a><a id="Approximate-construction-for-speed-1"></a><a class="docs-heading-anchor-permalink" href="#Approximate-construction-for-speed" title="Permalink"></a></h2><p>Generating model-X knockoffs scales as <span>$\mathcal{O}(p^3)$</span> with coordinate descent (e.g. <code>sdp_fast</code>, <code>mvr</code>, <code>maxent</code>), which becomes prohibitively slow for large <span>$p$</span> (e.g. <span>$p = 5000$</span>). </p><p>Sometimes one expects that covariates are only correlated with its nearby neighbors. Then, we can approximate the covariance matrix as a block diagonal structure with block size <code>windowsize</code>, and solve each block independently as smaller problems. This is implemented as <a href="https://biona001.github.io/Knockoffs.jl/dev/man/api/#Knockoffs.approx_modelX_gaussian_knockoffs">approx_modelX_gaussian_knockoffs</a></p><pre><code class="language-julia hljs">@time me_approx = approx_modelX_gaussian_knockoffs(X, :maxent, windowsize=100);</code></pre><pre><code class="nohighlight hljs">  4.548431 seconds (16.01 M allocations: 1.061 GiB, 4.17% gc time, 93.69% compilation time)</code></pre><h2 id="Multiple-knockoffs"><a class="docs-heading-anchor" href="#Multiple-knockoffs">Multiple knockoffs</a><a id="Multiple-knockoffs-1"></a><a class="docs-heading-anchor-permalink" href="#Multiple-knockoffs" title="Permalink"></a></h2><p><a href="http://proceedings.mlr.press/v89/gimenez19b.html">Gimenez et al</a> suggested multiple simultaneous knockoffs, which can give a boost in power when the target FDR or the number of variables to select are low. </p><p>If one generated <span>$m$</span> knockoffs for each of the <span>$p$</span> variables, the convex optimization problem in solving for diagonal <span>$S$</span> matrix is equally efficient as in the single-knockoff case, but the subsequent model selection would have <span>$(m + 1) * p$</span> columns as opposed to <span>$2p$</span> columns in the single-knockoff case. Thus, both computational speed and memory demand scales roughly linearly in <span>$m$</span>. </p><pre><code class="language-julia hljs">m = 5
@time me_multiple = modelX_gaussian_knockoffs(X, :maxent, μ, Σ, m=m);</code></pre><pre><code class="nohighlight hljs">  3.006152 seconds (2.68 k allocations: 775.460 MiB, 14.50% gc time, 0.17% compilation time)</code></pre><p>As a sanity check, lets make sure the modified SDP constraint is satisfied</p><pre><code class="language-julia hljs">eigmin((m+1)/m * Σ - Diagonal(me_multiple.s))</code></pre><pre><code class="nohighlight hljs">0.14792714564321696</code></pre><p>Finally, we can compare the <code>s</code> vector estimated from all 4 methods.</p><pre><code class="language-julia hljs">[me.s me_2nd_order.s me_approx.s me_multiple.s]</code></pre><pre><code class="nohighlight hljs">1000×4 Matrix{Float64}:
 0.760607  0.993382  0.846479  0.456365
 0.599795  0.83418   0.644484  0.359877
 0.611403  1.05053   0.787572  0.366842
 0.610539  0.958254  0.722984  0.366324
 0.610604  0.882614  0.690845  0.366362
 0.610599  0.896145  0.706702  0.366359
 0.610599  0.95216   0.715815  0.36636
 0.610599  0.858501  0.636454  0.36636
 0.610599  0.938511  0.70445   0.36636
 0.610599  0.921345  0.715783  0.36636
 0.610599  0.956845  0.698709  0.36636
 0.610599  0.97871   0.723473  0.36636
 0.610599  0.915263  0.688165  0.36636
 ⋮                             
 0.610599  0.917573  0.708208  0.36636
 0.610599  0.92724   0.712444  0.36636
 0.610599  0.982501  0.7634    0.36636
 0.610599  0.860957  0.680147  0.36636
 0.610599  1.02923   0.774822  0.36636
 0.610599  1.01452   0.751665  0.36636
 0.610599  0.941284  0.710888  0.366359
 0.610603  0.999975  0.755369  0.366362
 0.610539  0.86102   0.660549  0.366324
 0.611403  0.969511  0.786857  0.366842
 0.599795  0.861765  0.685225  0.359877
 0.760607  0.739856  0.613179  0.456364</code></pre><p>In this example, they are quite different.</p><h2 id="LASSO-example"><a class="docs-heading-anchor" href="#LASSO-example">LASSO example</a><a id="LASSO-example-1"></a><a class="docs-heading-anchor-permalink" href="#LASSO-example" title="Permalink"></a></h2><p>Let us apply the generated knockoffs to the model selection problem</p><blockquote><p>Given response <span>$\mathbf{y}_{n \times 1}$</span>, design matrix <span>$\mathbf{X}_{n \times p}$</span>, we want to select a subset <span>$S \subset \{1,...,p\}$</span> of variables that are truly causal for <span>$\mathbf{y}$</span>. </p></blockquote><h3 id="Simulate-data"><a class="docs-heading-anchor" href="#Simulate-data">Simulate data</a><a id="Simulate-data-1"></a><a class="docs-heading-anchor-permalink" href="#Simulate-data" title="Permalink"></a></h3><p>We will simulate </p><p class="math-container">\[\mathbf{y} \sim N(\mathbf{X}\mathbf{\beta}, \mathbf{\epsilon}), \quad \mathbf{\epsilon} \sim N(0, 1)\]</p><p>where <span>$k=50$</span> positions of <span>$\mathbf{\beta}$</span> is non-zero with effect size <span>$\beta_j \sim N(0, 1)$</span>. The goal is to recover those 50 positions using LASSO.</p><pre><code class="language-julia hljs"># set seed for reproducibility
Random.seed!(123)

# simulate true beta
n, p = size(X)
k = 50
βtrue = zeros(p)
βtrue[1:k] .= randn(k)
shuffle!(βtrue)

# find true causal variables
correct_position = findall(!iszero, βtrue)

# simulate y
y = X * βtrue + randn(n)</code></pre><pre><code class="nohighlight hljs">500-element Vector{Float64}:
 -16.138781364984787
  -5.287542411651397
  18.027692775987678
  -4.140794496081527
   4.223445068157524
   8.237597611236556
  -8.800473194308873
  12.138035964379569
  -5.291660763277003
   0.1763453121292271
  12.719488833739977
  -8.398513822600917
  -1.9198345300850481
   ⋮
  -3.771806541112785
   0.5681365432035446
  -0.5397047794787977
  -0.700940301452057
  -7.850480614685315
  -6.200339809747463
  -9.87399476750332
   2.219038357726496
   4.788196033460055
   4.61565606038031
  -5.4031821003145595
  -7.35857531035862</code></pre><h3 id="Standard-LASSO"><a class="docs-heading-anchor" href="#Standard-LASSO">Standard LASSO</a><a id="Standard-LASSO-1"></a><a class="docs-heading-anchor-permalink" href="#Standard-LASSO" title="Permalink"></a></h3><p>Lets try running standard LASSO. We use LASSO solver in <a href="https://github.com/JuliaStats/GLMNet.jl">GLMNet.jl</a> package, which is just a Julia wrapper for the GLMnet Fortran code. </p><p>How does it perform in power and FDR?</p><pre><code class="language-julia hljs"># run 10-fold cross validation to find best λ minimizing MSE
lasso_cv = glmnetcv(X, y)
λbest = lasso_cv.lambda[argmin(lasso_cv.meanloss)]

# use λbest to fit LASSO on full data
βlasso = glmnet(X, y, lambda=[λbest]).betas[:, 1]

# check power and false discovery rate
power = length(findall(!iszero, βlasso) ∩ correct_position) / k
FDR = length(setdiff(findall(!iszero, βlasso), correct_position)) / count(!iszero, βlasso)
println(&quot;Lasso power = $power, FDR = $FDR&quot;)</code></pre><pre><code class="nohighlight hljs">Lasso power = 0.96, FDR = 0.6619718309859155</code></pre><p>More than half of all Lasso discoveries are false positives. </p><h3 id="KnockoffLASSO"><a class="docs-heading-anchor" href="#KnockoffLASSO">Knockoff+LASSO</a><a id="KnockoffLASSO-1"></a><a class="docs-heading-anchor-permalink" href="#KnockoffLASSO" title="Permalink"></a></h3><p>Now lets try applying the knockoff methodology. Recall that consists of a few steps </p><ol><li>Run LASSO on <span>$[\mathbf{X} \mathbf{\tilde{X}}]$</span></li><li>Compare feature importance score <span>$W_j = \text{score}(x_j) - \text{score}(\tilde{x}_j)$</span> for each <span>$j = 1,...,p$</span>. Here we use <span>$W_j = |\beta_j| - |\tilde{\beta}_{j}|$</span></li><li>Choose target FDR <span>$q \in [0, 1]$</span> and compute </li></ol><p class="math-container">\[\tau = min_{t}\left\{t &gt; 0: \frac{{\{\#j: W_j ≤ -t}\}}{max(1, {\{\#j: W_j ≥ t}\})} \le q\right\}\]</p><p>The <a href="https://biona001.github.io/Knockoffs.jl/dev/man/api/#Knockoffs.fit_lasso"><code>fit_lasso</code></a> function generates knockoffs, run Lasso on <span>$[\mathbf{X} \mathbf{\tilde{X}}]$</span>, and apply knockoff filter.</p><pre><code class="language-julia hljs">@time knockoff_filter = fit_lasso(y, X, method=:maxent, m=1);</code></pre><pre><code class="nohighlight hljs">  3.506669 seconds (2.00 M allocations: 381.947 MiB, 1.03% gc time, 10.43% compilation time)</code></pre><p>The return type is now a <code>LassoKnockoffFilter</code>, which contains the following information</p><pre><code class="language-julia hljs">struct LassoKnockoffFilter{T} &lt;: KnockoffFilter
    y :: Vector{T} # n × 1 response vector
    X :: Matrix{T} # n × p matrix of original features
    ko :: Knockoff # A knockoff struct
    m :: Int # number of knockoffs per feature generated
    betas :: Vector{Vector{T}} # betas[i] is the p × 1 vector of effect sizes corresponding to fdr level fdr_target[i]
    a0 :: Vector{T}   # intercepts for each model in betas
    selected :: Vector{Vector{Int}} # selected[i] includes all variables selected based on target FDR level fdr_target[i]
    W :: Vector{T} # length p vector of feature importance
    taus :: Vector{T} # threshold for significance. For fdr fdr_target[i], tau[i] is threshold, and all W ≥ tau[i] is selected
    fdr_target :: Vector{T} # target FDR level for each taus and betas
    d :: UnivariateDistribution # distribution of y
    debias :: Union{Nothing, Symbol} # how betas and a0 have been debiased (`nothing` for not debiased)
end</code></pre><p>Lets do 10 simulations and visualize power and FDR trade-off:</p><pre><code class="language-julia hljs"># run 10 simulations and compute empirical power/FDR
nsims = 10
empirical_power = zeros(5)
empirical_fdr = zeros(5)
for i in 1:nsims
    @time knockoff_filter = fit_lasso(y, X, method=:mvr)
    for i in eachindex(knockoff_filter.fdr_target)
        selected = knockoff_filter.selected[i]
        power = length(selected ∩ correct_position) / k
        fdp = length(setdiff(selected, correct_position)) / max(length(selected), 1)
        empirical_power[i] += power
        empirical_fdr[i] += fdp
    end
end
empirical_power ./= nsims
empirical_fdr ./= nsims

# visualize FDR and power
power_plot = plot(FDR, empirical_power, xlabel=&quot;Target FDR&quot;, ylabel=&quot;Empirical power&quot;, legend=false, w=2)
fdr_plot = plot(FDR, empirical_fdr, xlabel=&quot;Target FDR&quot;, ylabel=&quot;Empirical FDR&quot;, legend=false, w=2)
Plots.abline!(fdr_plot, 1, 0, line=:dash)
plot(power_plot, fdr_plot)</code></pre><pre><code class="nohighlight hljs">  3.640497 seconds (40.55 k allocations: 286.264 MiB, 0.90% gc time, 0.32% compilation time)
  3.579325 seconds (907 allocations: 284.040 MiB, 0.21% gc time)
  3.590901 seconds (907 allocations: 284.040 MiB, 0.25% gc time)
  3.575671 seconds (907 allocations: 284.040 MiB, 0.23% gc time)
  3.570982 seconds (907 allocations: 284.040 MiB, 0.19% gc time)
  3.570007 seconds (907 allocations: 284.040 MiB, 0.15% gc time)
  3.577488 seconds (907 allocations: 284.040 MiB, 0.17% gc time)
  3.614974 seconds (907 allocations: 284.040 MiB, 0.18% gc time)
  3.567592 seconds (907 allocations: 284.040 MiB, 0.19% gc time)
  3.588125 seconds (907 allocations: 284.040 MiB, 0.13% gc time)</code></pre><p><img src="../output_29_1.png" alt="png"/></p><p><strong>Conclusion:</strong> </p><ul><li>LASSO + knockoffs controls the false discovery rate at below the target (dashed line). </li><li>The power of standard LASSO is better, but it comes with high empirical FDR that one cannot control via cross validation. </li><li>If one does not have the true mean and covariance of the <span>$p$</span> dimensional covariates, Knockoffs.jl will estimate them with sample mean and a shrunken (default = ledoit wolf) estimator. </li><li>Multiple simultaneous knockoffs increases power at the expensive of larger regression problem. </li><li>Approximate constructions can be leveraged for extremely large problems, e.g. <span>$p &gt; 10000$</span>. </li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../fixed/fixed/">« Fixed-X Knockoffs</a><a class="docs-footer-nextpage" href="../../group/">Group Knockoffs »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.9.0 on <span class="colophon-date" title="Thursday 20 March 2025 02:54">Thursday 20 March 2025</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
